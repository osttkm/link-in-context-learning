# Link-Context Learning for Multimodal LLMs
### requirement.txtã§torchãªã©ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹éš›ã¯cudatoolkitã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨å¯¾å¿œã—ãŸã‚‚ã®ã‚’è‡ªåˆ†ã§è¨­å®šã—ã¦ãã ã•ã„

## é€²æ—

1. [x] 11/8 ä¸€éƒ¨ã‚³ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹èª¬æ˜ã‚’è¿½åŠ ï¼Œã¾ãŸImagenetã«ã‚ˆã‚‹å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚’å®Œå‚™ï¼ç¾çŠ¶ã¯textã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã¯LLaVA v2ãŒï¼Œãã‚Œä»¥å¤–ã¯LLaVA v1ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚‰ã—ã„ï¼ä»Šå¾Œã®ãƒªãƒªãƒ¼ã‚¹æ¬¡ç¬¬ã§ã¯ã‚ã‚‹ãŒå°‘ã—ã‚³ãƒ¼ãƒ‰å¤‰ãˆã‚‹ã ã‘ã§ã„ã„ã‹ã‚‚ï¼ã¾ãŸå­¦ç¿’ã¯FSDPã‚‰ã—ã„ï¼
2. [x] 11/9 å­¦ç¿’ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦Gradioã§ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œï¼Œä¸€éƒ¨ã®ISEKAIãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã¯ã—ã£ã‹ã‚Šã¨LCLã§ãã¦ã„ã‚‹ï¼ã—ã‹ã—ã§ããªã„ã‚‚ã®ã‚‚å­˜åœ¨ã—ãŸï¼ã¾ãŸï¼ŒLCLã®æ€§èƒ½ã¯ã¾ã ã‚ˆã„ãŒVQAã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æ€§èƒ½ãŒè½ã¡ãŸã‚ˆã†ã«æ„Ÿã˜ã‚‹ï¼ã‚·ãƒ³ã‚°ãƒ«ã‚¿ã‚¹ã‚¯ã«ãªã£ã¨ã‚‹ï¼Ÿï¼Ÿã‚ˆã‚Šè©³ç´°ã«ï¼Œå­˜åœ¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦èª¿æŸ»ï¼Œã¾ãŸconfigå‘¨ã‚Šã‚‚ã—ã£ã‹ã‚Šã¨æŠŠæ¡ã™ã‚‹ï¼
3. [x] 11/15 è‡ªä½œãƒ‡ãƒ¼ã‚¿ã®jsonã‚’ä¿®æ­£ï¼Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯è¦ä¿®æ­£ã§ã¯ã‚ã‚‹ãŒæ­£ã—ãå­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼Qformerã«é–¢ã—ã¦ã®è¨˜è¿°ã¯ãªã„ãŒï¼Œè¨­å®šã™ã‚‹ã¨æ€§èƒ½ãŒã©ã®ã‚ˆã†ã«å¤‰ã‚ã‚‹ã®ã‹ï¼LLMå‰ã«ä¸€æ—¦ç‰¹å¾´é‡æ··ãœã¦ãŠã“ã†ã¨ã„ã†è€ƒãˆï¼Ÿï¼Ÿ
4. [x] 11/20 å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ï¼è‡ªä½œãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªï¼æ¤œè¨¼ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆï¼Œæ¤œè¨¼æ™‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã¾ã ã¡ã‚‡ã„ä¿®æ­£ãŒå¿…è¦
5. [x] 11/22 ACã§ã¯MVtecç”¨ã®æ¤œè¨¼ã‚³ãƒ¼ãƒ‰çµ‚äº†
6. [x] 11/27 ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ã«å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´
7. [x] 12/3 è£½å“ã‚ã¦ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã«è¿½åŠ ï¼ãƒãƒ¼ãƒãƒ«ï¼šæ™®é€šã«è£½å“ã«ã¤ã„ã¦æ•™ãˆã‚‹ã ã‘ã€€LCLãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è£½å“ã‚’æ•™ãˆã‚‹ï¼ã‚¯ã‚¨ãƒªã§ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã‚ã‚‹è£½å“ã‹ã©ã†ã‹ã‚’æ¨è«–

<p align="center" width="100%">
<img src="ISEKAI_overview.png"  width="80%" height="80%">
</p>

<div>
<div align="center">
    <a href='https://macavityt.github.io/' target='_blank'>Yan Tai<sup>*,1,2</sup></a>&emsp;
    <a href='https://weichenfan.github.io/Weichen/' target='_blank'>Weichen Fan<sup>*,â€ ,1</sup></a>&emsp;
    <a href='https://zhaozhang.net/' target='_blank'>Zhao Zhang<sup>1</sup></a>&emsp;
    <a href='https://zhufengx.github.io/' target='_blank'>Feng Zhu<sup>1</sup></a>&emsp;
    <a href='https://scholar.google.com/citations?user=1c9oQNMAAAAJ&hl=zh-CN' target='_blank'>Rui Zhao<sup>1</sup></a>&emsp;
    <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu<sup>&#x2709,3</sup></a>
</div>
<div>
<div align="center">
    <sup>1</sup>SenseTime Research&emsp;
    <sup>2</sup>Institute of Automation, CAS&emsp;
    <sup>3</sup>S-Lab, Nanyang Technological University&emsp;
    </br>
    <sup>*</sup> Equal Contribution&emsp;
    <sup>â€ </sup> Project Lead&emsp;
    <sup>&#x2709</sup> Corresponding Author
    
</div>
 
 -----------------

![](https://img.shields.io/badge/ISEKAI-v0.1-darkcyan)
![](https://img.shields.io/github/stars/isekai-portal/Link-Context-Learning)
![](https://black.readthedocs.io/en/stable/_static/license.svg)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fisekai-portal%2FLink-Context-Learning&count_bg=%23BDC4B7&title_bg=%2342C4A8&icon=octopusdeploy.svg&icon_color=%23E7E7E7&title=visitors&edge_flat=true)](https://hits.seeyoufarm.com)
[![Dataset](https://img.shields.io/badge/Dataset-Download-blue)](https://huggingface.co/ISEKAI-Portal) 
[![Generic badge](https://img.shields.io/badge/DEMO-LCL_Demo-<COLOR>.svg)](http://117.144.81.99:20488/)


## Updates
- **05 Sep, 2023**: :boom::boom: We release the code, data, and [LCL-2WAY-WEIGHT](https://huggingface.co/ISEKAI-Portal/LCL_2WAY_WEIGHT) checkpoint.
- **24 Aug, 2023**: :boom::boom: We release the online demo at [ğŸ”—LCL-DemoğŸ”—](http://117.144.81.99:20488/).
- **17 Aug, 2023**: :boom::boom: We release the two subsets of ISEKAI (ISEKAI-10 and ISEKAI-pair) at [[Hugging Face ğŸ¤—]](https://huggingface.co/ISEKAI-Portal).

---
This repository contains the **official implementation** and **dataset** of the following paper:

> **Link-Context Learning for Multimodal LLMs**<br>
> https://arxiv.org/abs/2308.07891
>
> **Abstract:** *The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to "learn to learn" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes "reasoning from cause and effect" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs.*

  
## Todo

1. [x] Release the [ISEKAI-10](https://huggingface.co/datasets/ISEKAI-Portal/ISEKAI-10) and [ISEKAI-pair](https://huggingface.co/datasets/ISEKAI-Portal/ISEKAI-pair).
2. [x] Release the dataset usage.
3. [x] Release the demo.
4. [x] Release the codes and checkpoints.
5. [ ] Release the full ISEKAI dataset.
6. [ ] Release checkpoints supporting few-shot detection and vqa tasks.


## Get Start

- [Install](#install)
- [Checkpoint](#checkpoint)
- [Dataset](#dataset)
- [Demo](#demo)

## Install

```shell
conda create -n lcl python=3.10
conda activate lcl
pip install -r requirements.txt
```

### configure accelerate

```shell
accelerate config
```
## Dataset

### ImageNet

We train the LCL setting on our rebuild ImageNet-900 set, and evaluate model on ImageNet-100 set. You can get the dataset json [here](https://github.com/isekai-portal/Link-Context-Learning/tree/main/docs).

### ISEKAI
We evaluate model on ISEKAI-10 and ISEKAI-Pair, you can download ISEKAI Dataset in [ISEKAI-10](https://huggingface.co/datasets/ISEKAI-Portal/ISEKAI-10) and [ISEKAI-pair](https://huggingface.co/datasets/ISEKAI-Portal/ISEKAI-pair).


## Checkpoint
Download our [LCL-2WAY-WEIGHT](https://huggingface.co/ISEKAI-Portal/LCL_2WAY_WEIGHT/tree/main) and [LCL-MIX](https://huggingface.co/ISEKAI-Portal/LCL-Mix) checkpoints in huggingface. 



## Demo

To launch a Gradio web demo, use the following command. Please note that the model evaluates in the torch.float16 format, which requires a GPU with at least 16GB of memory.

```shell
python ./mllm/demo/demo.py --model_path /path/to/lcl/ckpt
```

It is also possible to use it in 8-bit quantization, albeit at the expense of sacrificing some performance.

```shell
python ./mllm/demo/demo.py --model_path /path/to/lcl/ckpt --load_in_8bit
```

## Train

After preparing [data](https://github.com/shikras/shikra/blob/main/docs/data.md), you can train the model using the command:

### LCL-2Way-Weight
```shell
accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/lcl_train_2way_weight.py \
        --cfg-options data_args.use_icl=True \
        --cfg-options model_args.model_name_or_path=/path/to/init/checkpoint
```

### LCL-2Way-Mix
```shell
accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/lcl_train_mix1.py \
        --cfg-options data_args.use_icl=True \
        --cfg-options model_args.model_name_or_path=/path/to/init/checkpoint
```
## Inference

After preparing [data](#dataset), you can inference the model using the command:

### ImageNet-100
```shell
accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/lcl_eval_ISEKAI_10.py \
        --cfg-options data_args.use_icl=True \
        --cfg-options model_args.model_name_or_path=/path/to/checkpoint
```

mmengine style args and huggingface:Trainer args are supported. for example, you can change eval batchsize like this:

### ISEKAI
```shell
# ISEKAI10
accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/shikra_eval_multi_pope.py \
        --cfg-options data_args.use_icl=True \
        --cfg-options model_args.model_name_or_path=/path/to/checkpoint \
        --per_device_eval_batch_size 1

# ISEKAI-PAIR
accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/shikra_eval_multi_pope.py \
        --cfg-options data_args.use_icl=True \
        --cfg-options model_args.model_name_or_path=/path/to/checkpoint \
        --per_device_eval_batch_size 1
```

where `--cfg-options a=balabala b=balabala` is mmengine style argument. They will overwrite the argument predefined in config file. And `--per_device_eval_batch_size` is huggingface:Trainer argument.

the prediction result will be saved in `output_dir/multitest_xxxx_extra_prediction.jsonl`, which hold the same order as the input dataset. 

## Cite

```bibtex
@article{tai2023link,
  title={Link-Context Learning for Multimodal LLMs},
  author={Tai, Yan and Fan, Weichen and Zhang, Zhao and Zhu, Feng and Zhao, Rui and Liu, Ziwei},
  journal={arXiv preprint arXiv:2308.07891},
  year={2023}
}
```
