{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models as models\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from gradio.themes.utils.sizes import Size\n",
    "from PIL import Image\n",
    "from mmengine import Config\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "SLURM_ENV = {k: v for k, v in os.environ.items() if 'SLURM' in k}\n",
    "sys.path.append(str(Path(os.getcwd()).parent.parent))\n",
    "# sys.path.append(str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from mllm.models.builder.build_llava import load_pretrained_llava\n",
    "from mllm.dataset.process_function import PlainBoxFormatter\n",
    "from demo_dataset import prepare_demo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'logit_scale', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM device: cuda:0, is_quantized: False, is_loaded_in_4bit: False, is_loaded_in_8bit: False\n",
      "vision device: cuda:0, is_quantized: False, is_loaded_in_4bit: False, is_loaded_in_8bit: False\n"
     ]
    }
   ],
   "source": [
    "# TEMP_FILE_DIR = Path(__file__).parent / 'temp'\n",
    "TEMP_FILE_DIR = Path(os.getcwd()) / 'temp'\n",
    "TEMP_FILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#region paser\n",
    "\n",
    "model_path=r'/home/oshita/vlm/Link-Context-Learning/model_result/LCL_PG+VI'\n",
    "remove_model = False\n",
    "\n",
    "model_name_or_path = model_path\n",
    "vision_tower_path = r'/home/oshita/vlm/Link-Context-Learning/clip_vit_large_patch14.pt'\n",
    "#endregion\n",
    "\n",
    "#region configs\n",
    "model_args = dict(\n",
    "    type='llava',\n",
    "    # TODO: process version; current version use default version\n",
    "    version='v1',\n",
    "\n",
    "    # checkpoint config\n",
    "    cache_dir=None,\n",
    "    model_name_or_path=model_name_or_path,\n",
    "    vision_tower=vision_tower_path,\n",
    "    pretrain_mm_mlp_adapter=None,\n",
    "    # model config\n",
    "    mm_vision_select_layer=-2,\n",
    "    model_max_length=2048,\n",
    "    \n",
    "    # finetune config\n",
    "    freeze_backbone=False,\n",
    "    tune_mm_mlp_adapter=False,\n",
    "    freeze_mm_mlp_adapter=False,\n",
    "    freeze_mm_projector=False,\n",
    "\n",
    "    # data process config\n",
    "    is_multimodal=True,\n",
    "    sep_image_conv_front=False,\n",
    "    image_token_len=256,\n",
    "    mm_use_im_start_end=True,\n",
    "\n",
    "    target_processor=dict(\n",
    "        boxes=dict(type='PlainBoxFormatter'),\n",
    "    ),\n",
    "\n",
    "    process_func_args=dict(\n",
    "        conv=dict(type='LLavaConvProcessV1'),\n",
    "        target=dict(type='BoxFormatProcess'),\n",
    "        text=dict(type='LlavaTextProcessV2'),\n",
    "        image=dict(type='LlavaImageProcessorV1'),\n",
    "    ),\n",
    "\n",
    "    conv_args=dict(\n",
    "        conv_template=['causal_v1.0', 'hypnotized_ans_v1.0', 'final_v1.0', 'vicuna_v1.1'],\n",
    "        transforms=dict(type='Expand2square'),\n",
    "        tokenize_kwargs=dict(truncation_size=2048),\n",
    "    ),\n",
    "\n",
    "    gen_kwargs_set_pad_token_id=True,\n",
    "    gen_kwargs_set_bos_token_id=True,\n",
    "    gen_kwargs_set_eos_token_id=True,\n",
    ")\n",
    "model_args = Config(model_args)\n",
    "\n",
    "training_args = Config(dict(\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    device='cuda',\n",
    "    fsdp=None,\n",
    "))\n",
    "\n",
    "quantization_kwargs = dict()\n",
    "#region Load model and dataset\n",
    "if not remove_model:\n",
    "    model, preprocessor = load_pretrained_llava(model_args, training_args, **quantization_kwargs)\n",
    "    preprocessor['target'] = {'boxes': PlainBoxFormatter()}\n",
    "    tokenizer = preprocessor['text']\n",
    "\n",
    "    if not getattr(model, 'is_quantized', False):\n",
    "        model.to(dtype=torch.float16, device=torch.device('cuda'))\n",
    "    if not getattr(model.model.vision_tower[0], 'is_quantized', False):\n",
    "        model.model.vision_tower[0].to(dtype=torch.float16, device=torch.device('cuda'))\n",
    "\n",
    "    dataset_demo = prepare_demo_dataset(model_args=model_args, preprocessor=preprocessor)\n",
    "\n",
    "    print(f\"LLM device: {model.device}, is_quantized: {getattr(model, 'is_quantized', False)}, is_loaded_in_4bit: {getattr(model, 'is_loaded_in_4bit', False)}, is_loaded_in_8bit: {getattr(model, 'is_loaded_in_8bit', False)}\")\n",
    "    print(f\"vision device: {model.model.vision_tower[0].device}, is_quantized: {getattr(model.model.vision_tower[0], 'is_quantized', False)}, is_loaded_in_4bit: {getattr(model, 'is_loaded_in_4bit', False)}, is_loaded_in_8bit: {getattr(model, 'is_loaded_in_8bit', False)}\")\n",
    "else:\n",
    "    print(f'Skip model process.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vqa_state():\n",
    "    return {\n",
    "        'mode' : 'vqa',\n",
    "        'infer_img': [],\n",
    "        'infer_q': []\n",
    "    }\n",
    "    \n",
    "def state_update(state, key, value):\n",
    "    if value is None:\n",
    "        return\n",
    "    # format inputs\n",
    "    if isinstance(value, str):\n",
    "        special_tokens = [' <question>',' <image>', '<im_start>', '<im_end>', '[BEGIN EXAMPLE]', '[END EXAMPLE]', '[FINAL QUESTION]']\n",
    "        for token in special_tokens:\n",
    "            value = value.replace(token, '')\n",
    "    state[key].append(value)\n",
    "\n",
    "def predict(data_meta,class_name,idx,img_path):\n",
    "    if len(data_meta['infer_q']) == 0:\n",
    "        raise Exception('Please input question.')\n",
    "    \n",
    "    dataset_demo.update_data(data_meta)\n",
    "    model_inputs = dataset_demo[0]\n",
    "    print(f'=====model_inupts=====\\n\\n {model_inputs}')  \n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    model_inputs['images'] = model_inputs['images'].to(model_dtype)\n",
    "    print(f'=====model_inupts[\"image\"]=====\\n\\n {model_inputs}')  \n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    input_ids = model_inputs['input_ids']\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(dtype=torch.float16, device_type='cuda'):\n",
    "            outputs = model.generate(**model_inputs, **gen_kwargs, return_dict_in_generate=True, output_scores=True)\n",
    "            output_ids = outputs.sequences\n",
    "\n",
    "            transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "            generated_tokens = outputs.sequences[:, input_ids.shape[-1]:]\n",
    "            import numpy as np\n",
    "            for tok, score, full_score in zip(generated_tokens[0], transition_scores[0], outputs.scores):\n",
    "                full_score = full_score[0]\n",
    "                topk_softmax_score, topk_index = full_score.softmax(dim=-1).topk(5)\n",
    "                topk_origin_score = full_score[topk_index]\n",
    "                topk_tokens = tokenizer.convert_ids_to_tokens(topk_index)\n",
    "                topk_strs = [f\"[{idx:5d} | {token:8s} | {oscore:.3f} | {sscore:.2%}]\" for idx, token, oscore, sscore in zip(topk_index, topk_tokens, topk_origin_score, topk_softmax_score)]\n",
    "\n",
    "    input_token_len = input_ids.shape[-1]\n",
    "    response = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n",
    "    print(f\"response: {response}\")\n",
    "\n",
    "def get_image_feature(path:str):\n",
    "    state = init_vqa_state()\n",
    "\n",
    "    img_path = path\n",
    "    infer_imgbox = Image.open(img_path)\n",
    "\n",
    "    infer_q = 'What is the color of the cat?'\n",
    "    state_update(state, 'infer_img', infer_imgbox)\n",
    "    state_update(state, 'infer_q', infer_q)\n",
    "\n",
    "    dataset_demo.update_data(state)\n",
    "    model_inputs = dataset_demo[0]\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    model_inputs['images'] = model_inputs['images'].to(model_dtype)\n",
    "    model_inputs['images'] = model_inputs['images'].reshape((1,3,224,224))\n",
    "\n",
    "    return model.model.vision_tower[0](model_inputs['images'],output_hidden_states=True)\n",
    "\n",
    "def cos_sim(v1, v2, mean=True,dim=1):\n",
    "    cos = nn.CosineSimilarity(dim=dim, eps=1e-6)\n",
    "    if mean:\n",
    "        cos_sim = cos(v1, v2)\n",
    "    else:\n",
    "        combined = torch.cat((v1, v2), dim=0)\n",
    "        # print(combined.shape)\n",
    "        average = torch.mean(combined)\n",
    "        std = torch.std(combined)\n",
    "        _v1,_v2 = (v1-average)/std, (v2-average)/std\n",
    "        cos_sim = cos(_v1, _v2)\n",
    "    return cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dtype = next(model.parameters()).dtype\n",
    "\"\"\"CLIPの中間特徴量を射影するmlpの重みを取得\"\"\"\n",
    "mm_projector = nn.Linear(1024, 4096)\n",
    "weights = torch.load(model_path+'/pytorch_model-00003-of-00003.bin', map_location='cpu')\n",
    "mm_projector_weights = weights['model.mm_projector.weight']\n",
    "mm_projector.bias.data = weights['model.mm_projector.bias']\n",
    "mm_projector.weight.data = mm_projector_weights\n",
    "mm_projector.to(device='cuda').to(model_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP特徴の類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 4096])\n",
      "=====平均=====\n",
      "0.400146484375\n",
      "0.400146484375\n",
      "=====最大最小=====\n",
      "0.927734375\n",
      "-0.02117919921875\n",
      "0.927734375\n",
      "-0.02117919921875\n",
      "=====閾値=====\n",
      "T_above_threshold: 0.204345703125\n",
      "F_above_threshold: 0.20361328125\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "normal_data_path = r'/home/dataset/mvtec/hazelnut/test/good/000.png'\n",
    "defect_data_path = r'/home/dataset/mvtec/hazelnut/test/crack/000.png'\n",
    "\n",
    "good_feature = get_image_feature(normal_data_path).hidden_states[-2]\n",
    "good_feature = good_feature[:, 1:]\n",
    "good_feature = mm_projector(good_feature)\n",
    "\n",
    "defect_feature = get_image_feature(defect_data_path).hidden_states[-2]\n",
    "defect_feature = defect_feature[:, 1:]\n",
    "defect_feature = mm_projector(defect_feature)\n",
    "print(defect_feature.shape)\n",
    "# print(good_feature.max().item(), good_feature.min().item())\n",
    "# print(defect_feature.max().item(), defect_feature.min().item())\n",
    "\n",
    "print('=====平均=====')\n",
    "print(cos_sim(good_feature, defect_feature,mean=False).mean().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=True).mean().item())\n",
    "print('=====最大最小=====')\n",
    "print(cos_sim(good_feature, defect_feature,mean=False).max().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=True).min().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=False).max().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=True).min().item())\n",
    "print(\"=====閾値=====\")\n",
    "threshold = 0.5\n",
    "T_above_threshold = torch.sum(cos_sim(good_feature, defect_feature,mean=True) > threshold).item() / cos_sim(good_feature, defect_feature,mean=True).numel()\n",
    "F_above_threshold = torch.sum(cos_sim(good_feature, defect_feature,mean=False) > threshold).item() / cos_sim(good_feature, defect_feature,mean=False).numel()\n",
    "print(f'T_above_threshold: {T_above_threshold}')\n",
    "print(f'F_above_threshold: {F_above_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenetで事前学習したモデル特徴の類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = models.resnet50(pretrained=True).eval()\n",
    "transform_compose = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====平均=====\n",
      "0.543789267539978\n",
      "0.3592729866504669\n",
      "=====最大最小=====\n",
      "1.0000001192092896\n",
      "0.0\n",
      "1.0000001192092896\n",
      "0.0\n",
      "=====閾値=====\n",
      "T_above_threshold: 0.3843470982142857\n",
      "F_above_threshold: 0.5895647321428571\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def extract_features_from_resnet50(model, data, layer_name='layer4'):\n",
    "    # 抽出するレイヤーを指定\n",
    "    return_nodes = {layer_name: 'features'}\n",
    "    # 特徴抽出モデルを作成\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "    # 特徴量の抽出\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(data)\n",
    "    return features['features']\n",
    "\n",
    "normal_data_path = r'/home/dataset/mvtec/hazelnut/test/good/000.png'\n",
    "defect_data_path = r'/home/dataset/mvtec/hazelnut/test/crack/000.png'\n",
    "\n",
    "normal_data = Image.open(normal_data_path)\n",
    "normal_data = transform_compose(normal_data)\n",
    "normal_data = normal_data.unsqueeze(0)\n",
    "normal_data = normal_data.to(device)\n",
    "\n",
    "defect_data = Image.open(defect_data_path)\n",
    "defect_data = transform_compose(defect_data)\n",
    "defect_data = defect_data.unsqueeze(0)\n",
    "defect_data = defect_data.to(device)\n",
    "\n",
    "good_feature = extract_features_from_resnet50(model, normal_data,layer_name='layer3').squeeze(0)\n",
    "defect_feature = extract_features_from_resnet50(model, defect_data,layer_name='layer3').squeeze(0)\n",
    "\n",
    "\n",
    "print('=====平均=====')\n",
    "print(cos_sim(good_feature, defect_feature,mean=False).mean().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=True).mean().item())\n",
    "print('=====最大最小=====')\n",
    "print(cos_sim(good_feature, defect_feature,mean=False).max().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=True).min().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=False).max().item())\n",
    "print(cos_sim(good_feature, defect_feature,mean=True).min().item())\n",
    "print(\"=====閾値=====\")\n",
    "threshold = 0.5\n",
    "F_above_threshold = torch.sum(cos_sim(good_feature, defect_feature,mean=False) > threshold).item() / cos_sim(good_feature, defect_feature,mean=False).numel()\n",
    "T_above_threshold = torch.sum(cos_sim(good_feature, defect_feature,mean=True) > threshold).item() / cos_sim(good_feature, defect_feature,mean=True).numel()\n",
    "print(f'F_above_threshold: {F_above_threshold}')\n",
    "print(f'T_above_threshold: {T_above_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正常画像と欠陥画像の類似度(平均値)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7593, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "tensor(0.7842, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "tensor(0.7593, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "tensor(0.3032, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m defect_paths:\n\u001b[0;32m---> 24\u001b[0m     defect_feature \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     25\u001b[0m     defect_feature \u001b[38;5;241m=\u001b[39m defect_feature[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     26\u001b[0m     defect_feature \u001b[38;5;241m=\u001b[39m mm_projector(defect_feature)\n",
      "Cell \u001b[0;32mIn[34], line 68\u001b[0m, in \u001b[0;36mget_image_feature\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     65\u001b[0m state_update(state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer_q\u001b[39m\u001b[38;5;124m'\u001b[39m, infer_q)\n\u001b[1;32m     67\u001b[0m dataset_demo\u001b[38;5;241m.\u001b[39mupdate_data(state)\n\u001b[0;32m---> 68\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_demo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     69\u001b[0m model_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     70\u001b[0m model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model_dtype)\n",
      "File \u001b[0;32m~/vlm/Link-Context-Learning/mllm/demo/demo_dataset.py:105\u001b[0m, in \u001b[0;36mSingleImageInteractive.__getitem__\u001b[0;34m(self, index, debug_mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index, debug_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 105\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     update_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    108\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/vlm/Link-Context-Learning/mllm/dataset/single_image_convsation.py:106\u001b[0m, in \u001b[0;36mSingleImageConvDatasetMixin.__getitem__\u001b[0;34m(self, index, debug_mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m             res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__getitem_origin__(index,debug_mode,data\u001b[38;5;241m=\u001b[39minfo)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitem_icl__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__getitem_origin__(index,debug_mode)\n",
      "File \u001b[0;32m~/vlm/Link-Context-Learning/mllm/dataset/single_image_convsation.py:129\u001b[0m, in \u001b[0;36mSingleImageConvDatasetMixin.__getitem_icl__\u001b[0;34m(self, index, debug_mode, data)\u001b[0m\n\u001b[1;32m    127\u001b[0m     sub_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_icl_item__(item, mode\u001b[38;5;241m=\u001b[39mconv_mode, eval_icl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     sub_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_icl_item__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m ret_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(sub_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# concatenate multi-context\u001b[39;00m\n",
      "File \u001b[0;32m~/vlm/Link-Context-Learning/mllm/dataset/single_image_convsation.py:86\u001b[0m, in \u001b[0;36mSingleImageConvDatasetMixin.__get_icl_item__\u001b[0;34m(self, item, do_mask, debug_mode, train_mode, mode, eval_icl)\u001b[0m\n\u001b[1;32m     84\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_conv(raw_conv,mode)\n\u001b[1;32m     85\u001b[0m text_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(conv, do_mask, eval_icl)\n\u001b[0;32m---> 86\u001b[0m image_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# return\u001b[39;00m\n\u001b[1;32m     89\u001b[0m ret_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/vlm/Link-Context-Learning/mllm/dataset/single_image_convsation.py:386\u001b[0m, in \u001b[0;36mSingleImageConvDatasetMixin.process_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: Image\u001b[38;5;241m.\u001b[39mImage) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m    convert Image.Image object to torch.Tensor\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_func\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vlm/Link-Context-Learning/mllm/dataset/process_function/llava_process_function.py:262\u001b[0m, in \u001b[0;36mLlavaImageProcessorV1.__call__\u001b[0;34m(self, image, preprocessor)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLava not support MultiImage\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m--> 262\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(image_processor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrop_size\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/develop/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:317\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# PIL RGBA images are converted to RGB\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb:\n\u001b[0;32m--> 317\u001b[0m     images \u001b[38;5;241m=\u001b[39m [convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/develop/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:317\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# PIL RGBA images are converted to RGB\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb:\n\u001b[0;32m--> 317\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/develop/lib/python3.10/site-packages/transformers/image_transforms.py:743\u001b[0m, in \u001b[0;36mconvert_to_rgb\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[0;32m--> 743\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m/opt/miniconda3/envs/develop/lib/python3.10/site-packages/PIL/Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    876\u001b[0m ):\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/develop/lib/python3.10/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize the dictionaries\n",
    "class_avg_similarities = {}\n",
    "defect_similarities = {}\n",
    "\n",
    "mvtec_names = ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "for name in mvtec_names:\n",
    "    defect_names = [p.name for p in Path(r'/home/dataset/mvtec/'+name+'/test').glob('*') if p.is_dir()]\n",
    "    defect_names.remove('good')\n",
    "    if 'combined' in defect_names:\n",
    "        defect_names.remove('combined')\n",
    "    for defect_name in defect_names:\n",
    "        # Get the feature of the middle layer\n",
    "        good_feature = get_image_feature(rf'/home/dataset/mvtec/{name}/test/good/000.png').hidden_states[-2]\n",
    "        # Remove the class token\n",
    "        good_feature = good_feature[:, 1:]\n",
    "        good_feature = mm_projector(good_feature)\n",
    "\n",
    "        defect_paths = [p for p in Path(rf'/home/dataset/mvtec/{name}/test/{defect_name}').glob('*.png')]\n",
    "        similarity_scores = []\n",
    "        for path in defect_paths:\n",
    "            defect_feature = get_image_feature(path).hidden_states[-2]\n",
    "            defect_feature = defect_feature[:, 1:]\n",
    "            defect_feature = mm_projector(defect_feature)\n",
    "            similarity_scores.append(cos_sim(good_feature, defect_feature))\n",
    "            # print(cos_sim(good_feature, defect_feature))\n",
    "\n",
    "        average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "        print(average_similarity)\n",
    "\n",
    "        # Save the average similarity for the class\n",
    "        class_avg_similarities[name] = average_similarity.item()  # Convert tensor to a Python number\n",
    "\n",
    "        # Save the similarity scores for each defect\n",
    "        if name not in defect_similarities:\n",
    "            defect_similarities[name] = {}\n",
    "        defect_similarities[name][defect_name] = [s.item() for s in similarity_scores]  # Convert each tensor to a Python number\n",
    "\n",
    "    # Save the dictionaries as .txt files in the specified path\n",
    "    save_path = f'/home/oshita/vlm/Link-Context-Learning/mllm/demo/result/mvtec_feature/{name}/{Path(model_path).name}'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(os.path.join(save_path, 'OKvsNO_class_avg_similarities.txt'), 'w') as f:\n",
    "        f.write(json.dumps(class_avg_similarities))\n",
    "    with open(os.path.join(save_path, 'OKvsNO_defect_similarities.txt'), 'w') as f:\n",
    "        f.write(json.dumps(defect_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle : 0.8037109375\n",
      "cable : 0.316650390625\n",
      "capsule : 0.49462890625\n",
      "carpet : 0.336181640625\n",
      "grid : 0.2705078125\n",
      "hazelnut : 0.468017578125\n",
      "leather : 0.317626953125\n",
      "metal_nut : 0.301025390625\n",
      "pill : 0.49560546875\n",
      "screw : 0.317626953125\n",
      "tile : 0.25244140625\n",
      "toothbrush : 0.4833984375\n",
      "transistor : 0.431884765625\n",
      "wood : 0.267578125\n",
      "zipper : 0.40478515625\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionaries\n",
    "good2good_class_avg_similarities = {}\n",
    "\n",
    "mvtec_names = ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "for name in mvtec_names:\n",
    "    good_feature = get_image_feature(rf'/home/dataset/mvtec/{name}/test/good/000.png').hidden_states[-2]\n",
    "    good_feature = good_feature[:, 1:]\n",
    "    good_feature = mm_projector(good_feature)\n",
    "    good_paths = [p for p in Path(rf'/home/dataset/mvtec/{name}/test/good').glob('*.png')]\n",
    "    good_paths.remove(good_paths[0])\n",
    "    similarity_scores = []\n",
    "    for path in good_paths:\n",
    "        good2_feature = get_image_feature(path).hidden_states[-2]\n",
    "        good2_feature = good2_feature[:, 1:]\n",
    "        good2_feature = mm_projector(good2_feature)\n",
    "        similarity_scores.append(cos_sim(good_feature, good2_feature))\n",
    "\n",
    "    average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "    print(f'{name} : {average_similarity}')\n",
    "\n",
    "    # Save the average similarity for the class\n",
    "    good2good_class_avg_similarities[name] = average_similarity.item()\n",
    "\n",
    "    save_path = f'/home/oshita/vlm/Link-Context-Learning/mllm/demo/result/mvtec_feature/{name}/{Path(model_path).name}'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(os.path.join(save_path, 'OKvsOK_class_avg_similarities.txt'), 'w') as f:\n",
    "        f.write(json.dumps(good2good_class_avg_similarities))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle : 0.044\n",
      "cable : 0.031\n",
      "capsule : 0.008\n",
      "carpet : 0.037\n",
      "grid : 0.038\n",
      "hazelnut : 0.001\n",
      "leather : 0.071\n",
      "metal_nut : 0.031\n",
      "pill : 0.004\n",
      "screw : 0.028\n",
      "tile : 0.036\n",
      "toothbrush : 0.031\n",
      "transistor : 0.145\n",
      "wood : 0.056\n",
      "zipper : 0.008\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for (OKvsNO_key,OKvsNO_value),(OKvsOK_key, OKvsOK_value) in zip(class_avg_similarities.items(),good2good_class_avg_similarities.items()):\n",
    "    print(f\"{OKvsNO_key} : {np.round(np.abs(OKvsNO_value-OKvsOK_value),3)}\")\n",
    "    save_path = f'/home/oshita/vlm/Link-Context-Learning/mllm/demo/result/mvtec_feature/{OKvsNO_key}/{Path(model_path).name}'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(os.path.join(save_path, 'diff_class_avg_similarities.txt'), 'w') as f:\n",
    "        f.write(f\"{OKvsNO_key} : {np.round(np.abs(OKvsNO_value-OKvsOK_value),3)}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_and_transform_image(image_path, size=(224, 224)):\n",
    "    # 画像の読み込み\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # 画像の前処理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def extract_features_from_resnet50(model,image_path, layer_name='layer4'):\n",
    "    return_nodes = {layer_name: 'features'}\n",
    "    \n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "    image = load_and_transform_image(image_path)\n",
    "    image = image.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(image)\n",
    "    return features['features']\n",
    "\n",
    "# 使用例\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.resnet50(pretrained=True).to(device).eval()\n",
    "good_path = '/home/dataset/mvtec/wood/test/good/000.png'  # 画像のパスを指定\n",
    "layer_to_extract = 'layer4'  # 抽出したいレイヤーを指定\n",
    "features = extract_features_from_resnet50(model,path, layer_to_extract)\n",
    "\n",
    "print(features.shape)  # 抽出された特徴量の形状を表示\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
